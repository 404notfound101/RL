{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD6XtO1RxkVx"
      },
      "source": [
        "# 1. Explain clearly why V_pi is not useful in the MC development above?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F92rrOGxnwx"
      },
      "source": [
        "Since in MC method, most likely we do not have the model of the environment. So we don't know the state transition probability. Therefore, with only V(s) we cannot make a decision which action should be taken. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiyvzT5a0wdx"
      },
      "source": [
        "# 2. The MC algorithm so far (ref: p 99), requires an infinite number of episodes for Eval to converge on Q_pi_k (step k). We can modify this algorithm to the practical variant where Eval is truncated (c.f., DynProg GPI). In this case:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDC6WVrO03H4"
      },
      "source": [
        "# a. Will we obtain Q_pi_k from eval?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_t48E4v05Vu"
      },
      "source": [
        "eventually, yes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whDKMReJ2t0Y"
      },
      "source": [
        "# b. If not why are we able to truncate Eval? Explain clearly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PtvAZpK2xur"
      },
      "source": [
        "The basic idea is that the truncate Eval provides improvement to Q_pi so that ideally a better policy can be drawn from this improved Q_pi. Therefore, even though truncate Eval would take more policy iteration steps, it would eventually converge to the optimal policy with a fit Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq118odH39z8"
      },
      "source": [
        "# c. Assuming ES (i.e., thorough sampling of the S x A space), and the above truncated Eval_trunc, is it possible to converge on a sub-optimal policy pi_c? Is this a stable fixed point of the GPI for MC? Explain clearly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIJyLSGZ4Ay9"
      },
      "source": [
        "No, when the policy is converged to improve very slowly based on the deficient Q from the truncated Eval, the truncated Eval will be repeated each iteration and become a complete Eval which gives a perfect Q_pi_c. If pi_c is not the optimal policy, the policy would still be inproved based on the perfect Q_pi_c."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTBMAAkR_SYV"
      },
      "source": [
        "# 3. Explain how you can synthesize a stochastic policy given what you know so far (you don't need to read ahead)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtYblji2_Xgz"
      },
      "source": [
        "Using a e-greedy policy with decay each episode on e so the policy eventually becomes a deterministic policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I3dn6Ss_0QO"
      },
      "source": [
        "# Code the algorithm for MC Control (Off Policy) and apply this to the Cart Pole problem. You must discretize the environmental feedback (S) in order to solve this problem properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gHnzbCBEPU6"
      },
      "source": [
        "from random import randint\n",
        "import math\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUOQzu_AEWca"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96TDqV0rrqLn"
      },
      "source": [
        "class MC():\n",
        "  def __init__(self):\n",
        "    self.stepsHistory = [0]*500000\n",
        "    self.value = {}\n",
        "    self.maxlifetime = 200\n",
        "    self.c = {}\n",
        "  \n",
        "  def act(self, state):\n",
        "    return np.argmax(self.value[state])\n",
        "\n",
        "\n",
        "  def behavier_policy(self):\n",
        "    return env.action_space.sample()\n",
        "\n",
        "  def mc_sample(self, obs, epicode_count):\n",
        "    episodeStatesActions = []\n",
        "    G = 0\n",
        "    w = 1\n",
        "    for t in range(self.maxlifetime):\n",
        "      state = self.getState(obs)\n",
        "      action = self.behavier_policy()\n",
        "      episodeStatesActions.append({'state': state, 'action': action})\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      G += reward\n",
        "      if done:\n",
        "        self.stepsHistory[epicode_count] = t\n",
        "        for i in range(len(episodeStatesActions))[::-1]:\n",
        "          state_action = episodeStatesActions[i]\n",
        "          self.update(state_action['state'], state_action['action'], G-i, w)\n",
        "          if state_action['action'] != self.act(state_action['state']):\n",
        "            break\n",
        "          w = w*2\n",
        "        break\n",
        "\n",
        "  def update(self, state, action, G, w):\n",
        "    if not state in self.c:\n",
        "      self.c[state] = []\n",
        "      for _ in range(env.action_space.n):\n",
        "        self.c[state].append(0)\n",
        "    if not state in self.value:\n",
        "      self.value[state] = []\n",
        "      for _ in range(env.action_space.n):\n",
        "        self.value[state].append(0)\n",
        "    self.c[state][action] += w\n",
        "    self.value[state][action] += (w / self.c[state][action]) * (G - self.value[state][action])\n",
        "\n",
        "\n",
        "  def getState(self,obs):\n",
        "    state = \"\"\n",
        "    for o in obs:\n",
        "      state += str(math.floor(o))\n",
        "    return state"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CEws3Ok365j"
      },
      "source": [
        "\"\"\"  def act(self,state):\n",
        "    maxValue = env.action_space.sample()\n",
        "    if not state in self.value:\n",
        "      self.value[state] = []\n",
        "      for _ in range(env.action_space.n):\n",
        "        self.value[state].append({'count':0, 'value':0})\n",
        "    for action in range(env.action_space.n):\n",
        "      if self.value[state][maxValue]['value'] < self.value[state][action]['value']:\n",
        "        \n",
        "        maxValue = action\n",
        "      return maxValue\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4eYTloaU__-"
      },
      "source": [
        "agent = MC()\n",
        "for epicode_count in range(500000):\n",
        "  obs = env.reset()\n",
        "  agent.mc_sample(obs,epicode_count)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOWas9gBXKQ1"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC8VPjuvXNlR"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MOiPveWXO5L",
        "outputId": "cffba715-eab6-4e02-e953-3db2628af107"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fed16c0e6d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "ZRH0XtxyW1A0",
        "outputId": "4395e105-1f11-4715-a158-2bffd964d1b5"
      },
      "source": [
        "\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "obs = env.reset()\n",
        "for i in range(50000):\n",
        "  state = agent.getState(obs)\n",
        "  action = agent.act(state)\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  \n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "    \n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations that were run: 126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}