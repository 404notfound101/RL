# -*- coding: utf-8 -*-
"""A4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/142ZRBvpf7cAr-pviKOfU49nfRkOxamO-
"""

import torch
from torch import nn as nn
from torch.utils import data
import numpy as np
import gym

"""# Environment"""

env = gym.make('CartPole-v0')

"""# NN model"""

class cart_pole(nn.Module):
  def __init__(self):
    super(cart_pole,self).__init__()
    self.fc1 = nn.Linear(4,64)
    self.fc3 = nn.Linear(64,64)
    self.fc2 = nn.Linear(64,2)

  def forward(self,s):
    output = torch.nn.functional.leaky_relu((self.fc1(s)))
    output = torch.nn.functional.leaky_relu((self.fc3(output)))
    output = self.fc2(output)
    return output

"""# hyperparameters and essential functions"""

class TD_0_cart_pole():
  def __init__(self):
    self.q = cart_pole()
    self.history = []
    self.gamma = 0.99
    self.w_e = 0.009
    self.max_e = 0.9
    self.loss = nn.MSELoss()
    self.optimizer = torch.optim.SGD(self.q.parameters(),lr=0.01, momentum=0.9)

  def act(self,s):
    a = torch.argmax(self.q(torch.from_numpy(s).float()))
    return a.item()
  
  def e_greedy(self,s,e):
    if np.random.rand() < e:
      a = env.action_space.sample()
    else:
      a = self.act(s)
    return a

  def decay(self,episode):
    e = max(-episode * self.w_e + self.max_e, 0.1/(episode + 1))
    return e

  def update(self,s,a,s1,a1,r):
    next_state_estimation = self.q(torch.from_numpy(s).float())[a]
    next_state_max = self.gamma * self.q(torch.from_numpy(s1).float())[a1] + r
    #print(next_state_estimation,next_state_max)
    l = self.loss(next_state_estimation,next_state_max)
    #print(l)
    self.optimizer.zero_grad()
    l.backward()
    #for param in self.q.parameters():
      #param.grad.data.clamp_(-1, 1)
    self.optimizer.step()
    return l

agent = TD_0_cart_pole()
n_episodes = 200

"""# Training"""

for episoda in range(n_episodes):
  obs = env.reset()
  e = agent.decay(episoda)
  #e = 0.1
  s = obs
  a = agent.e_greedy(s,e)
  G = 0
  L = 0
  #for t in range(500):
  while True:
    obs, r, done, _ = env.step(a)
    G += r
    s1 = obs
    a1 = agent.e_greedy(s1,e)
    l = agent.update(s,a,s1,a1,r)
    L += l
    if done:
      agent.history.append(G)
      #agent.policy_inprovement()
      print("Episoda: {} | Final return: {} | average loss: {}".format(episoda,G,L.item()/G))
      break
    s,a = s1,a1

"""# results"""

test = []
for i in range(10):
  obs = env.reset()
  s = obs
  a = agent.act(s)
  G = 0
  while True:
    obs, r, done, _ = env.step(a)
    G += r
    s1 = obs
    a = agent.act(s1)
    if done:
      test.append(G)
      break
print("SARSA | average score: {}".format(np.mean(test)))

"""# Environment (mountain_car)"""

env = gym.make('MountainCar-v0')

"""# NN mountain_car"""

class mountain_car(nn.Module):
  def __init__(self):
    super(mountain_car,self).__init__()
    self.fc1 = nn.Linear(2,32)
    self.fc3 = nn.Linear(32,32)
    self.fc2 = nn.Linear(32,3)

  def forward(self,s):
    output = nn.functional.leaky_relu(self.fc1(s))
    output = nn.functional.leaky_relu(self.fc3(output))
    output = self.fc2(output)
    return output

"""# Dataset class"""

class training(data.Dataset):
  def __init__(self,s,a,s1,a1,r):
    self.s = s
    self.a = a
    self.s1 = s1
    self.a1 = a1
    self.r = r

  def __len__(self):
    return self.s.shape[0]

  def __getitem__(self,idx):
    return self.s[idx],self.a[idx],self.s1[idx],self.a1[idx],self.r[idx]

"""# hyperparameters and functions mountain_car"""

#dual network training to handle potential problem from backward() and optimizer.step()
training_q = mountain_car()
target_q = mountain_car()
gamma = 0.9
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(training_q.parameters(),lr=0.01)
def act(s):
  a = torch.argmax(training_q(torch.from_numpy(s).float()))
  return a.item()
def e_greedy(s,e):
  if np.random.rand() < e:
    a = env.action_space.sample()
  else:
    a = act(s)
  return a

"""# Training"""

training_s = []
training_a = []
training_s1 = []
training_a1 = []
training_r = []
for episode in range(300):
  e = 0.8 - 0.0077*episode
  for i in range(10):
    s = env.reset()
    a = e_greedy(s,e)
    G = 0
    step = 0
    while True:
      s1, r, done, _ = env.step(a)
      a1 = e_greedy(s1,e)
      G += r
      if s1[0] > -0.2:
        r += 0.2
      elif s1[0] > -0.15:
        r += 0.5
      elif s1[0] > -0.1:
        r += 0.7
      training_s.append(s)
      training_a.append(a)
      training_s1.append(s1)
      training_a1.append(a1)
      training_r.append(r)
      step += 1 
      if done:
        if step < 160:
          print("Episode: {} | Final return: {}".format(episode,G))
          torch.save(training_q.state_dict(), "my" + str(step) + ".pth")
        break
      s, a = s1, a1
  if len(training_s) > 4000:
    training_s = training_s[-4000:]
    training_a = training_a[-4000:]
    training_s1 = training_s1[-4000:]
    training_a1 = training_a1[-4000:]
    training_r = training_r[-4000:]
  MyData = training(torch.FloatTensor(training_s),torch.LongTensor(training_a),torch.FloatTensor(training_s1),torch.LongTensor(training_a1),torch.LongTensor(training_r))
  loader = data.DataLoader(MyData,128,True)
  iter_times = 0
  training_q.train()
  target_q.eval()
  for epoch in range(1,11):
    for s,a,s1,a1,r in loader:
      iter_times += 1
      if iter_times % 100 == 0:
        target_q.load_state_dict(training_q.state_dict())       
      state_estimation = training_q(s)
      state_max = (r + torch.gather(target_q(s1), dim=1, index=a1.unsqueeze(1)).squeeze(1)) * gamma
      loss = criterion(torch.gather(state_estimation, dim=1, index=a.unsqueeze(1)).squeeze(1),state_max)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
    print("Episode: {} | Loss: {}".format(episode,loss))
  target_q.load_state_dict(training_q.state_dict())

"""# Results"""

control_mountain_car = mountain_car()
control_mountain_car.load_state_dict(torch.load("/content/my92.pth"))

def act_mountain_car(s):
  a = torch.argmax(control_mountain_car(torch.from_numpy(s).float()))
  return a.item()

test = []
for i in range(10):
  obs = env.reset()
  s = obs
  a = act_mountain_car(s)
  G = 0
  while True:
    obs, r, done, _ = env.step(a)
    G += r
    s1 = obs
    a = act_mountain_car(s1)
    if done:
      test.append(G)
      break
print("SARSA | average score: {}".format(np.mean(test)))